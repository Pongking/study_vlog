{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    '''\n",
    "    return output of Rectified Linear Unit and derivative\n",
    "    '''\n",
    "    f=np.maximum(0,x)\n",
    "    df=np.where(x>0,1,0)\n",
    "    return f,df\n",
    "def sigmoid(x):\n",
    "\n",
    "    f=1/(1+np.exp(-x))\n",
    "    df=f*(1-f)\n",
    "    return f,df\n",
    "def tanh(x):\n",
    "    f=-1+2/(1+np.exp(-2*x))\n",
    "    df=1-f**2\n",
    "    return f,df\n",
    "def identity(x):\n",
    "    '''\n",
    "    恒等函数\n",
    "    '''\n",
    "    f=x\n",
    "    df=1\n",
    "    return f,df\n",
    "def selu(x):\n",
    "    alpha=1.67326\n",
    "    lambd=1.05070\n",
    "    f=lambd*np.where(x>=0,x,alpha*(np.exp(x)-1))\n",
    "    df=lambd*np.where(x>=0,1,alpha*np.exp(x))\n",
    "    return f,df\n",
    "activation_table={\n",
    "    'relu':relu,\n",
    "    'sigmoid':sigmoid,\n",
    "    'tanh':tanh,\n",
    "    'identity':identity,\n",
    "    'selu':selu\n",
    "}\n",
    "#loss function\n",
    "def squared_error(y,yhat):\n",
    "    '''\n",
    "    input target predict\n",
    "    return mean squared error\n",
    "    '''\n",
    "    return np.sum(0.5*(y-yhat)**2),y-yhat\n",
    "def identity_loss(y,yhat):\n",
    "    return yhat,yhat\n",
    "\n",
    "loss_table={\n",
    "    'squared_error':squared_error,\n",
    "    'identity':identity_loss\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self,layer_dimensions,parameters):\n",
    "        #init the neuralnetwork weights\n",
    "        self.weights={}\n",
    "        for i in range(len(layer_dimensions)-1):\n",
    "            self.weights[i]=np.random.uniform(-0.1,0.1,\n",
    "                                            (layer_dimensions[i],layer_dimensions[i+1]))\n",
    "            \n",
    "        self.learning_rate=parameters['learning_rate']\n",
    "        self.num_iteration=parameters['num_iteration']\n",
    "        self.batch_size=parameters['batch_size']\n",
    "\n",
    "        activation_name=parameters['activation']\n",
    "        if isinstance(activation_name,str) and activation_name in activation_table:\n",
    "            self.activation=activation_table[activation_name]\n",
    "        else:\n",
    "            self.activation=activation_name\n",
    "        loss_name=parameters['loss']\n",
    "        if isinstance(loss_name,str) and loss_name in loss_table:\n",
    "            self.loss=loss_table['loss_name']\n",
    "        else:\n",
    "            self.loss=loss_name\n",
    "    \n",
    "    def feedforward(self,x):\n",
    "        #feed forward\n",
    "        self.layer_input={}\n",
    "        #the output of the previous layer and the input of the next layer\n",
    "        self.layer_output={0:x}\n",
    "        for i in range(len(self.weights)):\n",
    "            self.layer_input[i]=np.dot(self.layer_output[i],self.weights[i])\n",
    "            self.layer_output[i+1]=self.activation(self.layer_input[i])[0]\n",
    "        return self.layer_output[len(self.weights)]\n",
    "    #input is calculated by the inner product of the output of i and weight\n",
    "    #output is calculated by activating the input \n",
    "    def backpropagation(self,y,yhat):\n",
    "        num_layers=len(self.weights)\n",
    "        #the output of last layer is predict\n",
    "        delta=-1*self.loss(y,yhat)[1]*self.activation(self.layer_input[num_layers-1])[1]\n",
    "        #initialize the last layer gradient weights\n",
    "        gradient_weights={num_layers-1:np.dot(self.layer_output[num_layers-1].T,delta)}\n",
    "        for i in reversed(range(num_layers-1)):\n",
    "            delta=np.dot(delta,self.weights[i+1].T)*self.activation(self.layer_input[i])[1]\n",
    "            gradient_weights[i]=np.dot(self.layer_output[i].T,delta)\n",
    "        return gradient_weights\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pc_demo1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
