{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    '''\n",
    "    return output of Rectified Linear Unit and derivative\n",
    "    '''\n",
    "    f=np.maximum(0,x)\n",
    "    df=np.where(x>0,1,0)\n",
    "    return f,df\n",
    "def sigmoid(x):\n",
    "\n",
    "    f=1/(1+np.exp(-x))\n",
    "    df=f*(1-f)\n",
    "    return f,df\n",
    "def tanh(x):\n",
    "    f=-1+2/(1+np.exp(-2*x))\n",
    "    df=1-f**2\n",
    "    return f,df\n",
    "def identity(x):\n",
    "    '''\n",
    "    恒等函数\n",
    "    '''\n",
    "    f=x\n",
    "    df=1\n",
    "    return f,df\n",
    "def selu(x):\n",
    "    alpha=1.67326\n",
    "    lambd=1.05070\n",
    "    f=lambd*np.where(x>=0,x,alpha*(np.exp(x)-1))\n",
    "    df=lambd*np.where(x>=0,1,alpha*np.exp(x))\n",
    "    return f,df\n",
    "activation_table={\n",
    "    'relu':relu,\n",
    "    'sigmoid':sigmoid,\n",
    "    'tanh':tanh,\n",
    "    'identity':identity,\n",
    "    'selu':selu\n",
    "}\n",
    "#loss function\n",
    "def squared_error(y,yhat):\n",
    "    '''\n",
    "    input target predict\n",
    "    return mean squared error\n",
    "    '''\n",
    "    return np.sum(0.5*(y-yhat)**2),y-yhat\n",
    "def identity_loss(y,yhat):\n",
    "    return yhat,yhat\n",
    "\n",
    "loss_table={\n",
    "    'squared_error':squared_error,\n",
    "    'identity':identity_loss\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self,layer_dimensions,parameters):\n",
    "        #init the neuralnetwork weights\n",
    "        self.weights={}\n",
    "        for i in range(len(layer_dimensions)-1):\n",
    "            self.weights[i]=np.random.uniform(-0.1,0.1,\n",
    "                                            (layer_dimensions[i],layer_dimensions[i+1]))\n",
    "            \n",
    "        self.learning_rate=parameters['learning_rate']\n",
    "        self.num_iteration=parameters['num_iterations']\n",
    "        self.batch_size=parameters['batch_size']\n",
    "\n",
    "        activation_name=parameters['activation']\n",
    "        if isinstance(activation_name,str) and activation_name in activation_table:\n",
    "            self.activation=activation_table[activation_name]\n",
    "        else:\n",
    "            self.activation=activation_name\n",
    "        loss_name=parameters['loss']\n",
    "        if isinstance(loss_name,str) and loss_name in loss_table:\n",
    "            self.loss=loss_table[loss_name]\n",
    "        else:\n",
    "            self.loss=loss_name\n",
    "    \n",
    "    def feedforward(self,x):\n",
    "        #feed forward\n",
    "        self.layer_input={}\n",
    "        #the output of the previous layer and the input of the next layer\n",
    "        self.layer_output={0:x}\n",
    "        for i in range(len(self.weights)):\n",
    "            self.layer_input[i]=np.dot(self.layer_output[i],self.weights[i])\n",
    "            self.layer_output[i+1]=self.activation(self.layer_input[i])[0]\n",
    "        return self.layer_output[len(self.weights)]\n",
    "    #input is calculated by the inner product of the output of i and weight\n",
    "    #output is calculated by activating the input \n",
    "    def backpropagation(self,y,yhat):\n",
    "        num_layers=len(self.weights)\n",
    "        #the output of last layer is predict\n",
    "        delta=-1*self.loss(y,yhat)[1]*self.activation(self.layer_input[num_layers-1])[1]\n",
    "        #initialize the last layer gradient weights\n",
    "        gradient_weights={num_layers-1:np.dot(self.layer_output[num_layers-1].T,delta)}\n",
    "        for i in reversed(range(num_layers-1)):\n",
    "            delta=np.dot(delta,self.weights[i+1].T)*self.activation(self.layer_input[i])[1]\n",
    "            gradient_weights[i]=np.dot(self.layer_output[i].T,delta)\n",
    "        return gradient_weights\n",
    "    \n",
    "    def train(self,x_train,y_train):\n",
    "        for iteration in range(self.num_iteration):\n",
    "            if self.batch_size>0 and self.batch_size<x_train.shape[0]:\n",
    "                batch_indices=np.random.choice(range(x_train.shape[0]),self.batch_size,replace=False)\n",
    "                # print(x_train.iloc[[5,6]])\n",
    "                x_batch=x_train.iloc[batch_indices]\n",
    "                y_batch=y_train[batch_indices]\n",
    "            else:\n",
    "                x_batch=x_train\n",
    "                y_batch=y_train\n",
    "            # x_batch=x_train[:64][:]\n",
    "            # y_batch=y_train[:64][:]\n",
    "            y_hat=self.feedforward(x_batch)\n",
    "            gradient_weights=self.backpropagation(y_batch,y_hat)\n",
    "            for layer in range(len(self.weights)):\n",
    "                self.weights[layer]-=self.learning_rate*gradient_weights[layer]\n",
    "    def predict(self,x):\n",
    "        activation_value=x\n",
    "        for layer in range(len(self.weights)):\n",
    "            weight_sum=np.dot(activation_value,self.weights[layer])\n",
    "            activation_value=self.activation(weight_sum)[0]\n",
    "        return activation_value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adminroot/miniconda3/envs/pc_demo1/lib/python3.8/site-packages/sklearn/datasets/_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 784) (56000, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "mnist=fetch_openml(\"mnist_784\",version=1)\n",
    "x=mnist.data/255.0\n",
    "y=mnist.target.astype(int)\n",
    "encoder=LabelBinarizer()\n",
    "y_one_hot=encoder.fit_transform(y)\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y_one_hot,test_size=0.2,random_state=42)\n",
    "print(x_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set:85.04%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "layer_dimensions=[784,128,64,10]\n",
    "parameters={\n",
    "    'learning_rate':0.02,\n",
    "    'num_iterations':2000,\n",
    "    'batch_size':2,\n",
    "    'activation':'tanh',\n",
    "    'loss':'squared_error'\n",
    "}\n",
    "nn=NeuralNetwork(layer_dimensions,parameters)\n",
    "nn.train(x_train,y_train)\n",
    "y_pred=nn.predict(x_test)\n",
    "correct_predictions=np.argmax(y_pred,axis=1)==np.argmax(y_test,axis=1)\n",
    "accuarcy=np.mean(correct_predictions)\n",
    "print(f\"accuracy on test set:{accuarcy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pc_demo1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
