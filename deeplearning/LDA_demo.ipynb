{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "device=torch.device('cuda:3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA(nn.Module):\n",
    "    def __init__(self,num_docs,num_words,num_topics,batch_size):\n",
    "        super(LDA,self).__init__()\n",
    "        self.num_docs=num_docs\n",
    "        self.num_words=num_words\n",
    "        self.num_topics=num_topics\n",
    "        self.batch_size=batch_size\n",
    "\n",
    "        self.doc_topic_prior=torch.ones(num_topics)\n",
    "        self.topic_word_prior=torch.ones(num_words)\n",
    "        self.doc_topic_dist=nn.Parameter(torch.randn(num_docs,num_topics))#[100,10]\n",
    "        self.top_word_dist=nn.Parameter(torch.randn(num_topics,num_words))#[10,500]\n",
    "    def forward(self,doc_word_counts): #[8,500]\n",
    "        \n",
    "        # doc_word_topic_dist=torch.zeros(self.num_docs,self.num_words,self.num_topics)\n",
    "        doc_word_topic_dist = torch.zeros(self.batch_size, self.num_words, self.num_topics).to(device)\n",
    "        for d in range(self.batch_size):\n",
    "            prob=self.doc_topic_dist[d].unsqueeze(1) * self.top_word_dist.unsqueeze(0)\n",
    "            #[1,10,500]\n",
    "            # tmp=[doc_word_counts[d] for _ in range(10)]\n",
    "            # tmp=torch.stack(tmp,dim=0).to(device)\n",
    "            # print(prob.size())\n",
    "            # print(tmp.size())#[10,500]\n",
    "            # print((prob*tmp).size())\n",
    "            word_count=doc_word_counts[d].unsqueeze(0).unsqueeze(0).to(device)\n",
    "            doc_word_topic_dist[d]=(prob*word_count).squeeze(0).transpose(0,1)\n",
    "            # print(doc_word_topic_dist[d].size())\n",
    "        doc_word_topic_dist=doc_word_topic_dist/doc_word_topic_dist.sum(dim=2,keepdim=True)\n",
    "\n",
    "        return doc_word_topic_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 7, 7,  ..., 4, 0, 1],\n",
       "         [2, 1, 1,  ..., 1, 7, 6],\n",
       "         [1, 1, 2,  ..., 4, 8, 4],\n",
       "         ...,\n",
       "         [8, 4, 8,  ..., 0, 4, 9],\n",
       "         [2, 8, 4,  ..., 8, 0, 8],\n",
       "         [6, 3, 3,  ..., 7, 9, 9]]),\n",
       " torch.Size([100, 500]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_docs=100\n",
    "num_words=500\n",
    "num_topics=10\n",
    "doc_word_counts=torch.randint(0,num_topics,(num_docs,num_words))\n",
    "doc_word_counts,doc_word_counts.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda:3')\n",
    "model=LDA(num_docs,num_words,num_topics,8).to(device)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.00000000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 3., 7.,  ..., 6., 5., 5.],\n",
       "        [2., 3., 3.,  ..., 3., 5., 2.],\n",
       "        [6., 3., 5.,  ..., 4., 1., 0.],\n",
       "        ...,\n",
       "        [8., 2., 5.,  ..., 4., 5., 1.],\n",
       "        [4., 3., 2.,  ..., 4., 5., 4.],\n",
       "        [7., 8., 6.,  ..., 9., 2., 7.]], device='cuda:3')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_word_counts=doc_word_counts.to(device)\n",
    "doc_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 500])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_word_counts[0:8].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: nan\n",
      "Epoch [20/100], Loss: nan\n",
      "Epoch [30/100], Loss: nan\n",
      "Epoch [40/100], Loss: nan\n",
      "Epoch [50/100], Loss: nan\n",
      "Epoch [60/100], Loss: nan\n",
      "Epoch [70/100], Loss: nan\n",
      "Epoch [80/100], Loss: nan\n",
      "Epoch [90/100], Loss: nan\n",
      "Epoch [100/100], Loss: nan\n"
     ]
    }
   ],
   "source": [
    "batch_size=8\n",
    "num_epochs=100\n",
    "num_batches = num_docs // batch_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 批量计算\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        # print(doc_word_counts[start_idx:end_idx].size())\n",
    "        # 前向传播\n",
    "        doc_word_topic_dist = model(doc_word_counts[start_idx:end_idx])\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion(\n",
    "            doc_word_topic_dist.view(-1, num_topics).to(device),\n",
    "            doc_word_counts[start_idx:end_idx].view(-1).to(device)\n",
    "        )\n",
    "\n",
    "        # 反向传播和优化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 打印训练进度\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (500) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[135], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m end_idx \u001b[38;5;241m=\u001b[39m start_idx \u001b[38;5;241m+\u001b[39m batch_size\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m doc_word_topic_dist \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_word_counts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# 计算损失\u001b[39;00m\n\u001b[1;32m     65\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[1;32m     66\u001b[0m     doc_word_topic_dist\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_topics),\n\u001b[1;32m     67\u001b[0m     doc_word_counts[start_idx:end_idx]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     68\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/tc2p1/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tc2p1/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[135], line 25\u001b[0m, in \u001b[0;36mLDA.forward\u001b[0;34m(self, doc_word_counts)\u001b[0m\n\u001b[1;32m     22\u001b[0m doc_word_topic_dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_words, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_topics)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size):\n\u001b[1;32m     24\u001b[0m     doc_word_topic_dist[d] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoc_topic_dist\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopic_word_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdoc_word_counts\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 归一化doc-word-topic概率分布\u001b[39;00m\n\u001b[1;32m     31\u001b[0m doc_word_topic_dist \u001b[38;5;241m=\u001b[39m doc_word_topic_dist \u001b[38;5;241m/\u001b[39m doc_word_topic_dist\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (500) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义LDA模型类\n",
    "class LDA(nn.Module):\n",
    "    def __init__(self, num_docs, num_words, num_topics, batch_size):\n",
    "        super(LDA, self).__init__()\n",
    "        self.num_docs = num_docs\n",
    "        self.num_words = num_words\n",
    "        self.num_topics = num_topics\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # 初始化模型参数\n",
    "        self.doc_topic_prior = torch.ones(num_topics)\n",
    "        self.topic_word_prior = torch.ones(num_words)\n",
    "        self.doc_topic_dist = nn.Parameter(torch.randn(num_docs, num_topics))\n",
    "        self.topic_word_dist = nn.Parameter(torch.randn(num_topics, num_words))\n",
    "\n",
    "    def forward(self, doc_word_counts):\n",
    "        # 计算doc-word-topic概率分布\n",
    "        doc_word_topic_dist = torch.zeros(self.batch_size, self.num_words, self.num_topics)\n",
    "        for d in range(self.batch_size):\n",
    "            doc_word_topic_dist[d] = (\n",
    "                self.doc_topic_dist[d].unsqueeze(1) *\n",
    "                self.topic_word_dist.unsqueeze(0) *\n",
    "                doc_word_counts[d].unsqueeze(1)\n",
    "            )\n",
    "\n",
    "        # 归一化doc-word-topic概率分布\n",
    "        doc_word_topic_dist = doc_word_topic_dist / doc_word_topic_dist.sum(dim=2, keepdim=True)\n",
    "\n",
    "        return doc_word_topic_dist\n",
    "\n",
    "# 准备数据\n",
    "num_docs = 100   # 文档数量\n",
    "num_words = 500  # 单词数量\n",
    "num_topics = 10  # 主题数量\n",
    "batch_size = 8   # 批次大小\n",
    "doc_word_counts = torch.randint(0, 10, (num_docs, num_words))\n",
    "\n",
    "# 创建LDA模型实例\n",
    "model = LDA(num_docs, num_words, num_topics, batch_size)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练LDA模型\n",
    "num_epochs = 100\n",
    "num_batches = num_docs // batch_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 批量计算\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "\n",
    "        # 前向传播\n",
    "        doc_word_topic_dist = model(doc_word_counts[start_idx:end_idx])\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion(\n",
    "            doc_word_topic_dist.view(-1, num_topics),\n",
    "            doc_word_counts[start_idx:end_idx].view(-1)\n",
    "        )\n",
    "\n",
    "        # 反向传播和优化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 打印训练进度\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 打印主题-词汇分布\n",
    "topic_word_dist = torch.softmax(model.topic_word_dist, dim=1)\n",
    "for t in range(num_topics):\n",
    "    topic_words = topic_word_dist[t].topk(5).indices\n",
    "    print(f\"Topic {t+1}: \", topic_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tc2p1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
